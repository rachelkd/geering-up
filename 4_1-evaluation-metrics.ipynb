{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65bcee7",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Formula 1 is the world's fastest and most technologically advanced motorsport, where drivers and teams compete in high-speed races across the globe. Each race is packed with excitement, strategy, and luckily for us, data.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "### ðŸŽï¸ Predicting Formula 1 Driver Positions\n",
    "\n",
    "In this activity, you will explore a real-world Formula 1 dataset from [F1nalyze Formula 1 Datathon](https://www.kaggle.com/competitions/f1nalyze-datathon-ieeecsmuj/overview). This dataset contains detailed information about past F1 races, including:\n",
    "\n",
    "- Race results (`position`): Where each driver finished in each race\n",
    "- Driver and team details: Information about the drivers and teams (constructors) they race for\n",
    "- Race conditions: Data like starting grid position, points scored, laps completed, etc.\n",
    "\n",
    "Many factors (e.g., starting position, team strategy, weather) can influence the outcome of a race. By analyzing this data, we can try to predict where each driver might finish in future races.\n",
    "\n",
    "## Our Focus: Evaluation Metrics\n",
    "\n",
    "While building a model to predict race results is exciting, our main goal in this activity is to learn how to evaluate the performance of such models. In machine learning, evaluation metrics help us understand how well our predictions match the real outcomes. By experimenting with the F1 dataset, youâ€™ll see how different metrics (like accuracy, mean absolute error, and more) can tell different stories about model performance.\n",
    "\n",
    "## Data\n",
    "\n",
    "1. Download the dataset [here](https://www.kaggle.com/competitions/f1nalyze-datathon-ieeecsmuj/data?select=validation.csv)\n",
    "2. Move it into your [Google Drive](https://drive.google.com/drive/u/0/my-drive) in the \"Colab Notebooks\" folder\n",
    "\n",
    "- `resultId`: Unique identifier for the result.\n",
    "- `racerId`: Identifier for the race.\n",
    "- `driverId`: Identifier for the driver.\n",
    "- `constructorId`: Identifier for the constructor (team).\n",
    "- `number`: Car number of the driver.\n",
    "- `grid`: Starting position on the grid.\n",
    "- `position_x`: Position from where the driver started in the grid\n",
    "- `positionText_x`: Starting Position in text format (e.g., 1st, 2nd).\n",
    "- `positionOrder`: Order of starting position.\n",
    "- `points`: Points scored in the race.\n",
    "- `laps`: Total number of laps in the race.\n",
    "- `time_x`: Time taken in the fastest lap\n",
    "- `timetaken_in_millisec`: Time taken in milliseconds.\n",
    "- `fastestLap`: Lap number of the fastest lap.\n",
    "- `rank`: Rank in the race.\n",
    "- `fastestLapTime`: Time of the fastest lap.\n",
    "- `max_speed`: Maximum speed achieved during the race.\n",
    "- `statusId`: Identifier for the race status.\n",
    "- `year`: Year of the race.\n",
    "- `round`: Round number of the race.\n",
    "- `circuitId`: Identifier for the circuit.\n",
    "- `grand_prix`: Name of the Grand Prix.\n",
    "- `date`: Date of the race.\n",
    "- `time_y`: Time of the race .\n",
    "- `url_x`: URL associated with the race.\n",
    "- `fp1_date`: Date of the first practice session.\n",
    "- `fp1_time`: Time of the first practice session.\n",
    "- `fp2_date`: Date of the second practice session.\n",
    "- `fp2_time`: Time of the second practice session.\n",
    "- `fp3_date`: Date of the third practice session.\n",
    "- `fp3_time`: Time of the third practice session.\n",
    "- `quali_date`: Date of the qualifying session.\n",
    "- `quali_time`: Time of the qualifying session.\n",
    "- `sprint_date`: Date of the sprint race session.\n",
    "- `sprint_time`: Time of the sprint race session.\n",
    "- `driverRef`: Reference to the driver.\n",
    "- `driver_num`: Unique identifier for the driver.\n",
    "- `driver_code`: Code assigned to the driver.\n",
    "- `forename`: First name of the driver.\n",
    "- `surname`: Last name of the driver.\n",
    "- `dob`: Date of birth of the driver.\n",
    "- `nationality`: Nationality of the driver.\n",
    "- `url_y`: URL associated with the driver.\n",
    "- `driverStandingsId`: Identifier for driver standings.\n",
    "- `raceId_y`: Identifier for the race associated with the driver.\n",
    "- `points_y`: Points scored by the driver.\n",
    "- `position`: Final position for a particular lap. (Target Variable)\n",
    "- `positionText_y`: Position text description.\n",
    "- `wins`: Number of wins by the driver.\n",
    "- `constructorRef`: Reference to the constructor (team).\n",
    "- `company`: Company associated with the constructor.\n",
    "- `nationality_y`: Nationality of the constructor.\n",
    "- `url`: URL associated with the constructor.\n",
    "- `status`: Status related to the driver or constructor.\n",
    "- `result_driver_standing`: The unique id for the row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e36da3",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "We first have to do a few things before we can evaluate our models (of course)!\n",
    "\n",
    "1. Import libraries\n",
    "2. Get our data\n",
    "3. Clean our data\n",
    "4. Data preprocessing\n",
    "5. Creating and training our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all columns for table since we have a lot of columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set max number of datapoints to use\n",
    "NUM_DATAPOINTS = 15_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba00d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training and testing dataset\n",
    "%pip install gdown\n",
    "%pip install zipfile36\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Check if data/f1nalyze exists\n",
    "if not os.path.exists('data/f1nalyze'):\n",
    "    os.makedirs('data', exist_ok=True)  # Ensure data/ exists\n",
    "\n",
    "    # Download the zip file into data/\n",
    "    zip_path = 'data/f1nalyze.zip'\n",
    "    gdown.download(\n",
    "        'https://drive.google.com/uc?export=download&id=1mRepXFuLfaPNBAlOonoe2dsCqR16uih4',\n",
    "        zip_path,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    # Extract zip into data/f1nalyze/\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data/f1nalyze')\n",
    "\n",
    "    # Step 4: Remove the zip file\n",
    "    os.remove(zip_path)\n",
    "else:\n",
    "    print(\"data/f1nalyze already exists, skipping download and extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e88f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_data_path = \"data/f1nalyze/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbd912",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f1_data_path + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8538c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f1_data_path + \"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94027898",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of our data\n",
    "# Since our training data has over 2M samples, training will take a long time!\n",
    "# Our testing data has 353,762 samples\n",
    "train = train.sample(n=NUM_DATAPOINTS, random_state=42)\n",
    "test = test.sample(n=NUM_DATAPOINTS, random_state=42)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9c1d6",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf198287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns in the training dataset but not in testing set\n",
    "col_not_in_test = set(train.columns) - set(test.columns)\n",
    "\n",
    "print(\"Columns not in test:\")\n",
    "for col in col_not_in_test:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"\\N\" with NaN\n",
    "# so pandas recognizes them as missing values\n",
    "\n",
    "train.replace(\"\\\\N\", pd.NA, inplace=True)\n",
    "test.replace(\"\\\\N\", pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_empty = train.isnull().sum()\n",
    "train_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a633db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_empty = test.isnull().sum()\n",
    "test_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use relevant columns\n",
    "columns_to_include = [\"grid\", \"positionText_x\", \"points\", \"laps\", \"round\", \"nationality\", \"points_y\", \"position\", \"wins\", \"company\", \"status\"]\n",
    "\n",
    "train_clean = train[columns_to_include]\n",
    "\n",
    "print(\"Train dataset with only relevant columns:\")\n",
    "train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b566fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See unique values for relevant columns\n",
    "cols = [\"positionText_x\", \"company\", \"status\", \"nationality\"]\n",
    "\n",
    "for col in cols:\n",
    "    uniq = train_clean[col].unique()\n",
    "    print(f\"{col}:\\n\", uniq, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorial variables\n",
    "ordinal_cols = [\"positionText_x\"]\n",
    "nominal_cols = [\"nationality\", \"company\", \"status\"]\n",
    "numerical_cols = [col for col in train_clean.columns if col not in ordinal_cols and col not in nominal_cols and col != \"position\"]\n",
    "\n",
    "# Define position order\n",
    "position_order = [\n",
    "    ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', 'R', 'N', 'W', 'D', 'F', 'E']\n",
    "]\n",
    "\n",
    "# Create ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=position_order)\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ord\", ordinal_encoder, ordinal_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), nominal_cols),\n",
    "        (\"num\", \"passthrough\", numerical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8702f",
   "metadata": {},
   "source": [
    "### Training Our Model(s)\n",
    "\n",
    "We are going to use evaluation metrics to compare a Decision Tree vs. Random Forest.\n",
    "\n",
    "We are going to use a **pipeline** to sequentially apply a list of transforms and a final estimator i.e., apply our transformations and model fitting/training at the same time!\n",
    "\n",
    "Read the scikit-learn documentation on [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and  [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to see which hyperparameters you can adjust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a **pipeline** to combine preprocessing and modeling\n",
    "pipeline_decision_tree = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline_forest = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ba63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data\n",
    "X_train = train_clean.drop(columns=\"position\")\n",
    "y_train = train_clean[\"position\"]\n",
    "\n",
    "X_test = test.drop(columns=\"position\")\n",
    "y_test = test[\"position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f3b5a",
   "metadata": {},
   "source": [
    "### Test our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our models using .predict()\n",
    "y_pred_decision_tree = pipeline_decision_tree.predict(X_test)\n",
    "y_pred_forest = pipeline_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique drivers predicted to be in position 1 for Decision Tree\n",
    "print(\"Decision Tree - Drivers predicted for position 1:\")\n",
    "dt_pos1_mask = y_pred_decision_tree == 1\n",
    "dt_pos1_indices = X_test.index[dt_pos1_mask]\n",
    "\n",
    "dt_drivers = set()\n",
    "for idx in dt_pos1_indices:\n",
    "    forename = X_test.loc[idx, 'forename']\n",
    "    surname = X_test.loc[idx, 'surname']\n",
    "    dt_drivers.add(f\"{forename} {surname}\")\n",
    "\n",
    "for driver in sorted(dt_drivers):\n",
    "    print(driver)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print unique drivers predicted to be in position 1 for Random Forest\n",
    "print(\"Random Forest - Drivers predicted for position 1:\")\n",
    "rf_pos1_mask = y_pred_forest == 1\n",
    "rf_pos1_indices = X_test.index[rf_pos1_mask]\n",
    "\n",
    "rf_drivers = set()\n",
    "for idx in rf_pos1_indices:\n",
    "    forename = X_test.loc[idx, 'forename']\n",
    "    surname = X_test.loc[idx, 'surname']\n",
    "    rf_drivers.add(f\"{forename} {surname}\")\n",
    "\n",
    "for driver in sorted(rf_drivers):\n",
    "    print(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d88d5",
   "metadata": {},
   "source": [
    "### Visualize our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Extract the trained DecisionTreeClassifier from the pipeline\n",
    "dt_model = pipeline_decision_tree.named_steps['model']\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(40, 20))\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    filled=True,\n",
    "    feature_names=pipeline_decision_tree.named_steps['preprocess'].get_feature_names_out(),\n",
    "    max_depth=5,  # Change this for deeper/shallower trees\n",
    "    impurity=False,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the trained RandomForestClassifier from the pipeline\n",
    "rf_model = pipeline_forest.named_steps['model']\n",
    "\n",
    "# Pick one tree from the forest (e.g., the first one)\n",
    "estimator = rf_model.estimators_[0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_tree(\n",
    "    estimator,\n",
    "    filled=True,\n",
    "    feature_names=pipeline_forest.named_steps['preprocess'].get_feature_names_out(),\n",
    "    max_depth=3,\n",
    "    impurity=False,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Random Forest - Tree 0 Visualization (max_depth=3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c03d5",
   "metadata": {},
   "source": [
    "### Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a205977",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce91535",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0adc6",
   "metadata": {},
   "source": [
    "## Your Turn: Pick an Evaluation Metric\n",
    "\n",
    "Judging from our accuracy scores, both models seem to be performing poorly!\n",
    "\n",
    "Let's use some other evaluation metrics.\n",
    "\n",
    "1. Go visit scikit-learn's documentation on [different evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "2. Pick a 3-5 different evaluation metrics\n",
    "3. Use them to see how the models' scores change!\n",
    "\n",
    "### Question to Consider: Can we use any evaluation metric?\n",
    "\n",
    "- Is `position` a categorical or numerical variable?\n",
    "\n",
    "> Remember to import the evaluation metric and run the code to be able to use the functions from scikit-learn!\n",
    "\n",
    "```python\n",
    "# Example import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any evaluation metric you want to use below\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Score your models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134b896",
   "metadata": {},
   "source": [
    "### Tune Your Models\n",
    "\n",
    "Now, make your own pipelines (preprocessor and model) with tuned hyperparameters.\n",
    "\n",
    "Test your models using new evaluation metrics.\n",
    "\n",
    "To get you started, here is the code from above to create your own model.\n",
    "\n",
    "```python\n",
    "# Use a **pipeline** to combine preprocessing and modeling\n",
    "pipeline_decision_tree = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline_forest = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "```\n",
    "\n",
    "You will need to change some things about it to tune your hyperparameters. You can also use some other classification models to do so.\n",
    "\n",
    "See available hyperparameters for:\n",
    "\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create new pipelines/models with tuned hyperparameters\n",
    "\n",
    "\n",
    "# TODO: Evaluate your new models with evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
